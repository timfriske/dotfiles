#!/usr/bin/env bash

# AWS S3 Delete Objects command
# =============================
# Tim Friske <me@tifr.de>
#
# Deletes batches of up to 1,000 S3 objects per request via the low-
# level command `aws s3api delete-objects`. This is much faster than
# using the higher-level command `aws s3 rm` which is only able to
# delete one S3 object per invocation.
#
# Deps:: awk, aws, bash, cat, env, find, jq, less, mktemp, parallel, sh
# Deps:: sort

shopt -os nounset pipefail errexit errtrace

# Standard input stream of script must be connected to terminal in order
# to prompt the user.
if [ ! -t 0 ]; then
  echo >&2 stdin must be connected to terminal
  exit 1
fi

bucket="${1:?'bucket required'}"
# Plain-text listing with each S3 key on a separate line. Blank and
# duplicate lines are ignored.
#
# Example listing:
#   top-level-s3-object.csv
#   parent/child_s3-object.txt
s3_keys_text_file="${2:?'S3 keys text file required'}"

# Abort if file with S3 keys is not in plain-text format, i.e. contains
# binary data.
if ! grep --binary-files=without-match --regexp='.' --quiet -- "${s3_keys_text_file}"; then
  printf >&2 'file with S3 keys must be plain-text: %s\n' "${s3_keys_text_file}"
  exit 1
fi

# Create a temporary directory with the given bucket as a prefix and a
# random suffix as its name. It is used to store intermediate results
# such as the json files which are generated to list up to 1,000 S3 keys
# of objects to delete. Also GNU parallel stores its joblog file in this
# directory.
temp_dir="$(mktemp --directory --tmpdir="${PWD}" -- "${bucket}.XXXXXXXXXX")"
temp_dir="${temp_dir#"${PWD}/"}"
printf 'Temporary directory: %s\n' "${temp_dir}"

# Delete blank and duplicate lines. S3 objects can be deleted only once.
awk 'NF&&!a[$0]++' "${s3_keys_text_file}" > "${temp_dir}/unique_s3_keys.txt"

# From the given text file with S3 keys of objects to delete generate
# json files with up to 1,000 of these S3 keys. Produce as many json
# files as needed in order to not exceed the maximum number of objects
# which the `aws s3api delete-objects` command can handle at once, i.e.
# per HTTP request.
cat "${temp_dir}/unique_s3_keys.txt" \
  | parallel --max-lines 1000 --max-args 1 --keep-order --pipe -- \
    sh -c "cat | jq --raw-input '{Key:.}' | jq --slurp '{Objects:.,Quiet:true}' > '${temp_dir}/{#}.json'"

echo ==========================================================================
echo WARNING: CAREFULLY REVIEW THE S3 OBJECTS BEFORE THEY WILL BE DELETED
echo WARNING: AFTER REVIEW YOU ARE ASKED AGAIN WHETHER YOU WANT TO CONTINUE
echo ==========================================================================
printf 'Bucket: %s\n' "${bucket}"
read -n 1 -s -r -p 'Press CTRL-C to abort or any other key to continue...'
echo

find "${temp_dir}" -type f -name '*.json' \
  | sort --numeric-sort \
  | xargs --no-run-if-empty -- jq --raw-output '.Objects[].Key' -- \
  | less

echo ==========================================================================
echo WARNING: YOU ARE ABOUT TO DELETE THE S3 OBJECTS YOU JUST REVIEWED
echo WARNING: ARE YOU ABSOLUTELY SURE TO IRREVERSIBLY DELETE THE S3 OBJECTS
echo ==========================================================================
printf 'Bucket: %s\n' "${bucket}"
printf 'Temporary directory: %s\n' "${temp_dir}"
read -n 1 -s -r -p 'Press CTRL-C to abort or any other key to continue...'
echo

# For every previously produced json file run as many processes of the
# `aws s3api delete-objects` command in parallel as there are threads
# in order to maximize the number of S3 objects which are deleted per
# time unit.
find "${temp_dir}" -type f -name '*.json' \
  | sort --numeric-sort \
  | parallel --max-args 1 --keep-order --joblog "${temp_dir}/joblog" --verbose -- \
    aws s3api delete-objects --bucket "${bucket}" --delete 'file://{}'
